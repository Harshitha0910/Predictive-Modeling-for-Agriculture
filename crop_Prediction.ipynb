{"cells":[{"source":"import pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, balanced_accuracy_score\n\n# Load the dataset\ncrops = pd.read_csv(\"soil_measures.csv\")\n\n# Check for missing values\nmissing_values = crops.isnull().sum()\nprint(\"Missing values:\\n\", missing_values)\n\n# Get unique crop types\nunique_crop_types = crops['crop'].unique()\nprint(\"Unique crop types:\\n\", unique_crop_types)\n\n# Define the target variable\ntarget = 'crop'\n\n# Split the data into features and target variable\nX = crops.drop(columns=[target])  # Features\ny = crops[target]                 # Target variable\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Define the features to be used in the model\nfeatures = [\"N\", \"P\", \"K\", \"ph\"]\n\n# Create an empty dictionary to store predictive performance of each feature\nfeatures_dict = {}\n\n# Loop through each feature to build a model\nfor feature in features:\n    # Isolate the current feature for model training\n    X_train_feature = X_train[[feature]]\n    X_test_feature = X_test[[feature]]\n\n    # Create a logistic regression model set for multinomial multi-class classification\n    log_reg = LogisticRegression(multi_class=\"multinomial\", max_iter=500)  # Increase max_iter if needed\n\n    # Fit the model on the training data using only the selected feature\n    log_reg.fit(X_train_feature, y_train)\n\n    # Predict the target values using the test set\n    y_pred = log_reg.predict(X_test_feature)\n\n    # Calculate the F1 score and balanced accuracy score\n    f1 = f1_score(y_test, y_pred, average=\"weighted\")\n    balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n\n    # Store the results in the dictionary\n    features_dict[feature] = {\"F1 Score\": f1, \"Balanced Accuracy\": balanced_accuracy}\n\n# Print the performance for each feature\nfor feature, scores in features_dict.items():\n    print(f\"F1-score for {feature}: {scores['F1 Score']:.4f}\")\n    print(f\"Balanced Accuracy for {feature}: {scores['Balanced Accuracy']:.4f}\")\n\n# Define which metric to compare, e.g., \"F1 Score\"\ncomparison_metric = \"F1 Score\"\n\n# Use a lambda function to fetch the specific metric from nested dictionaries for comparison\nbest_feature = max(features_dict, key=lambda x: features_dict[x][comparison_metric])\nbest_score = features_dict[best_feature][comparison_metric]\n\n# Create a dictionary to store the best predictive feature and its score\nbest_predictive_feature = {best_feature: best_score}\n\n# Print the best predictive feature and its score\nprint(\"Best Predictive Feature:\", best_predictive_feature)\n\n# Train the model using the entire dataset for the best feature\nbest_feature_model = LogisticRegression(multi_class=\"multinomial\", max_iter=500)\nbest_feature_model.fit(crops[[best_feature]], crops[target])\n\n# Function to predict crop type based on input values\ndef predict_crop(N, P, K, ph):\n    input_data = pd.DataFrame([[N, P, K, ph]], columns=[\"N\", \"P\", \"K\", \"ph\"])\n    prediction = best_feature_model.predict(input_data[[best_feature]])\n    return prediction[0]\n\n# Example usage\nN = float(input(\"Enter value for N: \"))\nP = float(input(\"Enter value for P: \"))\nK = float(input(\"Enter value for K: \"))\nph = float(input(\"Enter value for ph: \"))\n\npredicted_crop = predict_crop(N, P, K, ph)\nprint(\"Predicted crop type:\", predicted_crop)\n","metadata":{},"cell_type":"code","id":"f999b030-c306-4859-995b-b04ba0a675fe","outputs":[],"execution_count":null}],"metadata":{"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"editor":"DataLab"},"nbformat":4,"nbformat_minor":5}